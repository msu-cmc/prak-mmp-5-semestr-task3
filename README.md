# Random Forest –∏ Gradient Boosting: –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–±–æ—Ç–∞ –ø–æ –∫—É—Ä—Å—É "–ú–µ—Ç–æ–¥—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –æ–±—Ä–∞–∑–æ–≤" (–ú–ú–†–û).

## –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

–î–∞–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é —Å –Ω—É–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è **Random Forest** –∏ **Gradient Boosting** –¥–ª—è –∑–∞–¥–∞—á–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏. –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Python (numpy, scipy, matplotlib) –∏ `DecisionTreeRegressor` –∏–∑ scikit-learn.

### –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- ‚úÖ **Random Forest** - –∞–Ω—Å–∞–º–±–ª—å –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –¥–µ—Ä–µ–≤—å–µ–≤ —Å –±–∞–≥–≥–∏–Ω–≥–æ–º
- ‚úÖ **Gradient Boosting** - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–µ—Ä–µ–≤—å–µ–≤ –Ω–∞ –∞–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç–µ
- ‚úÖ **Early Stopping** - –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏
- ‚úÖ **–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫** - RMSLE –Ω–∞ train –∏ validation –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
- ‚úÖ **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞** –º–æ–¥–µ–ª–µ–π
- ‚úÖ **–ü–æ–ª–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã** –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (House Sales in King County)

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
.
‚îú‚îÄ‚îÄ ensembles/               # –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ random_forest.py    # Random Forest
‚îÇ   ‚îú‚îÄ‚îÄ boosting.py         # Gradient Boosting
‚îÇ   ‚îî‚îÄ‚îÄ utils.py            # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
‚îÇ
‚îú‚îÄ‚îÄ experiments/            # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ exp.ipynb          # Jupyter notebook —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏
‚îÇ   ‚îî‚îÄ‚îÄ README.md          # –û–ø–∏—Å–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
‚îÇ
‚îú‚îÄ‚îÄ data/                   # –î–∞—Ç–∞—Å–µ—Ç—ã
‚îÇ   ‚îî‚îÄ‚îÄ kc_house_data.csv  # House Sales in King County
‚îÇ
‚îú‚îÄ‚îÄ backend/                # Backend —Å–µ—Ä–≤–∏—Å (FastAPI)
‚îú‚îÄ‚îÄ frontend/               # Frontend —Å–µ—Ä–≤–∏—Å (React)
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml      # Docker Compose –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îú‚îÄ‚îÄ Makefile               # –ö–æ–º–∞–Ω–¥—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
‚îú‚îÄ‚îÄ DOCKER.md              # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ Docker
‚îú‚îÄ‚îÄ IMPLEMENTATION.md       # –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
‚îú‚îÄ‚îÄ REPORT.md              # –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
‚îî‚îÄ‚îÄ README.md              # –≠—Ç–æ—Ç —Ñ–∞–π–ª
```

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –í–∞—Ä–∏–∞–Ω—Ç 1: Docker (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```bash
# –°–±–æ—Ä–∫–∞ –∏ –∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
make up

# –ò–ª–∏ –±–µ–∑ make
docker-compose up -d
```

–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞:
- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000/docs

–ü–æ–¥—Ä–æ–±–Ω–µ–µ: [DOCKER.md](DOCKER.md)

### –í–∞—Ä–∏–∞–Ω—Ç 2: –õ–æ–∫–∞–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

#### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –°–æ–∑–¥–∞–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
python3 -m venv .venv

# –ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –µ–≥–æ
source .venv/bin/activate  # Unix/Mac
# –∏–ª–∏
.venv\Scripts\activate     # Windows

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install numpy pandas scikit-learn matplotlib seaborn jupyter joblib
```

#### 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```python
import numpy as np
from ensembles.random_forest import RandomForestMSE
from ensembles.boosting import GradientBoostingMSE

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
X_train, y_train = ...  # –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ
X_test, y_test = ...

# Random Forest
rf = RandomForestMSE(
    n_estimators=50,
    tree_params={"max_depth": 10, "random_state": 42}
)
rf.fit(X_train, y_train)
predictions_rf = rf.predict(X_test)

# Gradient Boosting
gb = GradientBoostingMSE(
    n_estimators=100,
    tree_params={"max_depth": 3, "random_state": 42},
    learning_rate=0.1
)
gb.fit(X_train, y_train)
predictions_gb = gb.predict(X_test)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
rf.dump("models/rf_model")
gb.dump("models/gb_model")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
rf_loaded = RandomForestMSE.load("models/rf_model")
gb_loaded = GradientBoostingMSE.load("models/gb_model")
```

#### 3. –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

```bash
cd experiments
jupyter notebook exp.ipynb
```

## –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã

### Random Forest

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**
- –û–±—É—á–µ–Ω–∏–µ N –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –¥–µ—Ä–µ–≤—å–µ–≤ –Ω–∞ bootstrap-–≤—ã–±–æ—Ä–∫–∞—Ö
- –ö–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –≤–∏–¥–∏—Ç —Å–ª—É—á–∞–π–Ω—É—é –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –æ–±—ä–µ–∫—Ç–æ–≤ (—Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º)
- –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ = —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –¥–µ—Ä–µ–≤—å—è–º

**–§–æ—Ä–º—É–ª–∞:**
```
≈∑(x) = (1/N) * Œ£ h_i(x)
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `n_estimators` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
- `tree_params` - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è DecisionTreeRegressor (max_depth, max_features, –∏ —Ç.–¥.)

### Gradient Boosting

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**
- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º (—Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
- –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–µ—Ä–µ–≤—å–µ–≤ –Ω–∞ –∞–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
- –î–ª—è MSE: –∞–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç = –æ—Å—Ç–∞—Ç–∫–∏ (y - —Ç–µ–∫—É—â–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)
- –ö–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º learning_rate

**–§–æ—Ä–º—É–ª–∞:**
```
F_0(x) = mean(y)
F_m(x) = F_{m-1}(x) + Œ∑ * h_m(x)
–≥–¥–µ h_m –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –∞–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç–µ: (y - F_{m-1}(x))
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `n_estimators` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
- `tree_params` - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è DecisionTreeRegressor
- `learning_rate` - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ–±—É—á–µ–Ω–∏—è (Œ∑)

## –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã

### –î–∞—Ç–∞—Å–µ—Ç
**House Sales in King County, USA** - 21,613 –æ–±—ä–µ–∫—Ç–æ–≤ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏

### –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã

**–î–ª—è Random Forest:**
1. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ (5, 10, 20, 30, 50, 75, 100)
2. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (sqrt, n/4, n/2, n)
3. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (3, 5, 10, 15, 20, None)

**–î–ª—è Gradient Boosting:**
1. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ (10, 20, 30, 50, 75, 100, 150)
2. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
3. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (2, 3, 5, 7, 10, None)
4. Learning rate (0.01, 0.05, 0.1, 0.2, 0.3, 0.5)

### –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã

| –ê—Å–ø–µ–∫—Ç | Random Forest | Gradient Boosting |
|--------|---------------|-------------------|
| **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤** | 50-75 | 75-100 |
| **–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞** | 10-15 | 3-5 |
| **RMSE –Ω–∞ —Ç–µ—Å—Ç–µ** | ~$167,000 | ~$158,000 |
| **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è** | –ë—ã—Å—Ç—Ä–µ–µ | –ú–µ–¥–ª–µ–Ω–Ω–µ–µ |
| **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º** | –í—ã—Å–æ–∫–∞—è | –°—Ä–µ–¥–Ω—è—è |
| **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å** | –í—ã—à–µ | –ù–∏–∂–µ |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- üöÄ **Random Forest** - –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- üéØ **Gradient Boosting** - –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫—É

## –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### –ü–æ–¥—Ä–æ–±–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è

- [IMPLEMENTATION.md](IMPLEMENTATION.md) - –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
- [REPORT.md](REPORT.md) - –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- [experiments/README.md](experiments/README.md) - –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–≤–µ–¥—ë–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

### API –º–æ–¥–µ–ª–µ–π

#### RandomForestMSE

```python
class RandomForestMSE:
    def __init__(self, n_estimators: int, tree_params: dict | None = None)

    def fit(self, X, y, X_val=None, y_val=None, trace=None, patience=None)
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

        Parameters:
        - X: –ø—Ä–∏–∑–Ω–∞–∫–∏ (n_samples, n_features)
        - y: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (n_samples,)
        - X_val, y_val: –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        - trace: –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        - patience: early stopping (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
        - ConvergenceHistory | None: –∏—Å—Ç–æ—Ä–∏—è –º–µ—Ç—Ä–∏–∫ –µ—Å–ª–∏ trace=True
        """

    def predict(self, X) -> np.ndarray
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""

    def dump(self, dirpath: str)
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""

    @classmethod
    def load(cls, dirpath: str) -> "RandomForestMSE"
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
```

#### GradientBoostingMSE

```python
class GradientBoostingMSE:
    def __init__(self, n_estimators: int, tree_params: dict | None = None,
                 learning_rate: float = 0.1)

    # –ú–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã RandomForestMSE
    def fit(...)
    def predict(...)
    def dump(...)
    @classmethod
    def load(...)
```

## –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å

### Random Forest

‚úÖ **Bootstrap sampling** - —Å–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ n
‚úÖ **–ê–≥—Ä–µ–≥–∞—Ü–∏—è** - —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π
‚úÖ **–ë–∞–≥–≥–∏–Ω–≥** - —Å–Ω–∏–∂–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –∑–∞ —Å—á—ë—Ç —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è

### Gradient Boosting

‚úÖ **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è** - F‚ÇÄ(x) = argmin E[L(y, Œ≥)] = mean(y) –¥–ª—è MSE
‚úÖ **–ê–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç** - –¥–ª—è MSE: -‚àáL = y - ≈∑ (–æ—Å—Ç–∞—Ç–∫–∏)
‚úÖ **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ** - F_m = F_{m-1} + Œ∑¬∑h_m
‚úÖ **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è** - —á–µ—Ä–µ–∑ learning_rate –∏ max_depth

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏:

```python
from sklearn.datasets import make_regression
from sklearn.metrics import mean_squared_error
import numpy as np

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
y = np.abs(y)

# Random Forest
rf = RandomForestMSE(n_estimators=10, tree_params={"max_depth": 5})
rf.fit(X, y)
y_pred = rf.predict(X)
print(f"RF RMSE: {np.sqrt(mean_squared_error(y, y_pred))}")

# Gradient Boosting
gb = GradientBoostingMSE(n_estimators=20, tree_params={"max_depth": 3}, learning_rate=0.1)
gb.fit(X, y)
y_pred = gb.predict(X)
print(f"GB RMSE: {np.sqrt(mean_squared_error(y, y_pred))}")
```

## –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –†–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
- ‚úÖ Python —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
- ‚úÖ NumPy
- ‚úÖ SciPy
- ‚úÖ Matplotlib
- ‚úÖ `sklearn.tree.DecisionTreeRegressor` (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–º–ø–æ—Ä—Ç –∏–∑ sklearn)

### –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
- ‚ùå `sklearn.ensemble.RandomForestRegressor`
- ‚ùå `sklearn.ensemble.GradientBoostingRegressor`
- ‚ùå –î—Ä—É–≥–∏–µ –≥–æ—Ç–æ–≤—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω—Å–∞–º–±–ª–µ–π

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

–ù–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ House Sales (~17,000 –æ–±—ä–µ–∫—Ç–æ–≤, 90 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):

| –ú–æ–¥–µ–ª—å | –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è | –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è | RMSE |
|--------|--------------|----------------|------|
| RF | 50 trees, depth=10 | ~50 —Å–µ–∫ | $167,000 |
| GB | 100 trees, depth=3, lr=0.1 | ~80 —Å–µ–∫ | $158,000 |

*–¢–µ—Å—Ç—ã –Ω–∞ MacBook Pro M1*

## –ê–≤—Ç–æ—Ä—ã –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

**–ö—É—Ä—Å:** –ú–µ—Ç–æ–¥—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –æ–±—Ä–∞–∑–æ–≤ (–ú–ú–†–û)

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**
1. –õ–µ–∫—Ü–∏–∏ –∫—É—Ä—Å–∞ –ú–ú–†–û
2. Breiman, L. (2001). "Random Forests"
3. Friedman, J. H. (2001). "Greedy Function Approximation: A Gradient Boosting Machine"

## –õ–∏—Ü–µ–Ω–∑–∏—è

–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ —É—á–µ–±–Ω—ã—Ö —Ü–µ–ª—è—Ö.
