# Эксперименты с Random Forest и Gradient Boosting

## Описание

Данная директория содержит эксперименты по исследованию поведения собственных реализаций алгоритмов Random Forest и Gradient Boosting на датасете House Sales in King County, USA.

## Файлы

- `exp.ipynb` - основной ноутбук с экспериментами
- `../data/kc_house_data.csv` - датасет с данными о продажах домов

## Структура экспериментов

### 1. Предобработка данных

**Выполненные шаги:**
- Извлечение признаков из даты (year, month, day)
- One-hot encoding для zipcode
- Разделение на train (64%), validation (16%), test (20%)
- Преобразование данных в numpy массивы

**Размер данных после предобработки:**
- ~17,000 объектов для обучения
- ~4,000 объектов для валидации
- ~4,000 объектов для тестирования
- ~90 признаков после one-hot encoding

### 2. Эксперименты с Random Forest

Исследованы следующие факторы:

#### 2.1. Количество деревьев (n_estimators)
- Диапазон: 5, 10, 20, 30, 50, 75, 100
- **Наблюдение:** Качество улучшается до 50-75 деревьев, затем прирост замедляется
- **Время:** Растёт линейно с количеством деревьев

#### 2.2. Размерность подвыборки признаков (max_features)
- Тестируемые значения: sqrt(n_features), n_features/4, n_features/2, n_features
- **Наблюдение:** Меньшие значения повышают разнообразие деревьев
- **Рекомендация:** sqrt(n_features) или n_features/3 обеспечивают хороший баланс

#### 2.3. Максимальная глубина дерева (max_depth)
- Диапазон: 3, 5, 10, 15, 20, None (неограниченная)
- **Наблюдение:**
  - Глубина 10-15 оптимальна
  - Неограниченная глубина приводит к переобучению и медленному обучению
  - Слишком мелкие деревья (3-5) недообучаются

### 3. Эксперименты с Gradient Boosting

Исследованы следующие факторы:

#### 3.1. Количество деревьев (n_estimators)
- Диапазон: 10, 20, 30, 50, 75, 100, 150
- **Наблюдение:** Качество продолжает улучшаться дольше, чем в RF
- **Рекомендация:** 75-150 деревьев в зависимости от learning_rate

#### 3.2. Размерность подвыборки признаков (max_features)
- Тестируемые значения: аналогично RF
- **Наблюдение:** Меньшее влияние, чем в RF, но всё ещё важно для регуляризации

#### 3.3. Максимальная глубина дерева (max_depth)
- Диапазон: 2, 3, 5, 7, 10, None
- **Наблюдение:**
  - **Оптимум: 3-5** (в отличие от RF!)
  - Мелкие деревья работают лучше в GB
  - Глубокие деревья приводят к переобучению

#### 3.4. Learning Rate
- Диапазон: 0.01, 0.05, 0.1, 0.2, 0.3, 0.5
- **Наблюдение:**
  - 0.1 - хороший баланс качества и скорости
  - <0.05 - требует больше итераций
  - >0.3 - ухудшает качество, слишком агрессивное обучение
- **Принцип:** Меньший LR + больше деревьев = лучше качество, но дольше обучение

## Запуск экспериментов

```bash
# Активируйте виртуальное окружение
source ../.venv/bin/activate  # для Unix/Mac
# или
..\.venv\Scripts\activate  # для Windows

# Запустите Jupyter Notebook
jupyter notebook exp.ipynb
```

## Результаты

### Лучшие конфигурации

**Random Forest:**
- n_estimators: 75
- max_depth: 15
- max_features: default

**Gradient Boosting:**
- n_estimators: 100
- max_depth: 5
- learning_rate: 0.1
- max_features: default

### Сравнение моделей

| Метрика | Random Forest | Gradient Boosting |
|---------|---------------|-------------------|
| RMSE | ~170,000 | ~160,000 |
| Время обучения | Быстрее | Медленнее |
| Устойчивость | Высокая | Средняя |
| Интерпретируемость | Выше | Ниже |

## Выводы

1. **Random Forest:**
   - Быстрее обучается
   - Менее чувствителен к гиперпараметрам
   - Хорошо параллелится
   - Подходит для быстрого прототипирования

2. **Gradient Boosting:**
   - Лучшее качество предсказаний
   - Требует тщательной настройки
   - Последовательное обучение (не параллелится)
   - Подходит для production с оптимизацией качества

3. **Общее:**
   - Оба алгоритма показывают хорошее качество на реальных данных
   - Реализации соответствуют классическим алгоритмам из теории
   - Поддерживают сохранение/загрузку моделей
   - Early stopping работает корректно

## Математические основы

### Random Forest
```
ŷ(x) = (1/T) * Σ h_t(x)
```
где T - количество деревьев, обученных на bootstrap-выборках

### Gradient Boosting
```
ŷ(x) = const + η * Σ h_t(x)
h_t обучается на антиградиенте: -∇L = (y - ŷ_{t-1})
```
где η - learning rate, const - начальное предсказание

## Дополнительные материалы

- [IMPLEMENTATION.md](../IMPLEMENTATION.md) - детальное описание реализации
- [ensembles/](../ensembles/) - исходный код реализаций
- [data/](../data/) - датасет для экспериментов
